+++
title = "Latent Model"
author = ["Fangyuan"]
date = 2020-10-31
slug = "20201031123814-latent-model"
tags = ["rl", "mbrl"]
draft = false
+++

## Description {#description}

模型学习的好坏还与环境的复杂性有关。有些环境是部分可观测的，状态是高维的且存在较多冗余信息。
这节将考虑如何从结构设计的角度使得模型的学习变得简单容易。
我们需要考虑训练的模型有：

<style>.org-center { margin-left: auto; margin-right: auto; text-align: center; }</style>

<div class="org-center">

{{< figure src="/img/rl-sergey/lec-11-9.png" width="100%" >}}

</div>

在全部可观测模型中，我们通常做的是对网络的输出做最大似然化：
\\[
\max \_{\phi} \frac{1}{N} \sum\_{i=1}^{N} \sum\_{t=1}^{T} \log p\_{\phi}\left(\mathbf{s}\_{t+1, i} \mid \mathbf{s}\_{t, i}, \mathbf{a}\_{t, i}\right)
\\]
但现在我们没有 \\(s\_t\\) ，只能对其期望做最大似然化：
\\[
\text { latent space model: } \max \_{\phi} \frac{1}{N} \sum\_{i=1}^{N} \sum\_{t=1}^{T} E\left[\log p\_{\phi}\left(\mathbf{s}\_{t+1, i} \mid \mathbf{s}\_{t, i}, \mathbf{a}\_{t, i}\right)+\log p\_{\phi}\left(\mathbf{o}\_{t, i} \mid \mathbf{s}\_{t, i}\right)\right]
\\]
其中，
\\[
\text { expectation w.r.t. }\left(\mathbf{s}\_{t}, \mathbf{s}\_{t+1}\right) \sim p\left(\mathbf{s}\_{t}, \mathbf{s}\_{t+1} \mid \mathbf{o}\_{1: T}, \mathbf{a}\_{1: T}\right)
\\]
这个分布需要这么复杂吗？

<style>.org-center { margin-left: auto; margin-right: auto; text-align: center; }</style>

<div class="org-center">

{{< figure src="/img/rl-sergey/lec-11-10.png" width="100%" >}}

</div>

这里我们使用简单的方法，使用：
\\[
\text { expectation w.r.t. } \mathbf{s}\_{t} \sim q\_{\psi}\left(\mathbf{s}\_{t} \mid \mathbf{o}\_{t}\right), \mathbf{s}\_{t+1} \sim q\_{\psi}\left(\mathbf{s}\_{t+1} \mid \mathbf{o}\_{t+1}\right)
\\]
为了简化讨论，这里的 observation model 是确定性的，
\\[
q\_{\psi}\left(\mathbf{s}\_{t} \mid \mathbf{o}\_{t}\right)=\delta\left(\mathbf{s}\_{t}=g\_{\psi}\left(\mathbf{o}\_{t}\right)\right) \Rightarrow \mathbf{s}\_{t}=g\_{\psi}\left(\mathbf{o}\_{t}\right)
\\]
我们的目标函数也就转换为：
\\[
\max \_{\phi, \psi} \frac{1}{N} \sum\_{i=1}^{N} \sum\_{t=1}^{T} \log p\_{\phi}\left(g\_{\psi}\left(\mathbf{o}\_{t+1, i}\right) \mid g\_{\psi}\left(\mathbf{o}\_{t, i}\right), \mathbf{a}\_{t, i}\right)+\log p\_{\phi}\left(\mathbf{o}\_{t, i} \mid g\_{\psi}\left(\mathbf{o}\_{t, i}\right)\right)
\\]
最后，在目标函数中加入关于奖励函数的优化：
\\[
\max \_{\phi, \psi} \frac{1}{N} \sum\_{i=1}^{N} \sum\_{t=1}^{T} \log p\_{\phi}\left(g\_{\psi}\left(\mathbf{o}\_{t+1, i}\right) \mid g\_{\psi}\left(\mathbf{o}\_{t, i}\right), \mathbf{a}\_{t, i}\right)+\log p\_{\phi}\left(\mathbf{o}\_{t, i} \mid g\_{\psi}\left(\mathbf{o}\_{t, i}\right)\right)+\log p\_{\phi}\left(r\_{t, i} \mid g\_{\psi}\left(\mathbf{o}\_{t, i}\right)\right)
\\]
整理一下，得到算法：

<style>.org-center { margin-left: auto; margin-right: auto; text-align: center; }</style>

<div class="org-center">

{{< figure src="/img/rl-sergey/lec-11-11.png" width="100%" >}}

</div>


## Links to this note {#links-to-this-note}

-   [Model Based RL]({{< relref "20201031101403-model_based_rl.md" >}})

    >   无模型的强化学习不需要知道环境动力学方程，只要不断的进行试错和更新策略的过程就可以很好的进行训练了。
    > 但基于模型的强化学习需要用到环境动力学方程，
    > 在此基础上，对未来进行规划，从而达到加速训练和提高训练结果的效果。
-   [SOLAR]({{< relref "20201031161623-solar.md" >}})

    > [Latent Model]({{< relref "20201031123814-latent_model.md" >}})

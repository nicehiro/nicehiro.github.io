+++
title = "KL Divergence"
author = ["Fangyuan"]
date = 2020-10-31
slug = "20201031155541-kl-divergence"
tags = ["math"]
draft = false
+++

KL 散度是两个概率分布 \\(P\\) 和 \\(Q\\) 差别的非对称性度量。
KL 散度是用来度量使用基于 \\(Q\\) 的分布来编码服从 \\(P\\) 的分布的样本 所需的 **额外** 的平均比特数。
典型情况下， \\(P\\) 表示数据的真实分布， \\(Q\\) 表示数据的理论分布或 \\(P\\) 的近似分布。

\begin{align\*}
D\_{KL}(P||Q)& = \mathcal{H}(P,Q) - H(P) \\\\
&=-\sum\_{i}P(i)\log{Q\_{i}}-(-\sum\_{i}P(i)\log{P(i)}) \\\\
&=\sum\_{i}P(i)\log\frac{P(i)}{Q(i)}
\end{align\*}


## Links to this note {#links-to-this-note}

-   [Kullback-Leibler Divergence Explained]({{< relref "20201101153127-kullback_leibler_divergence_explained.md" >}})

    > Mathematics description can check [KL Divergence]({{< relref "20201031155541-kl_divergence.md" >}}).
-   [Description]({{< relref "20201031125537-guided_search_method_v1.md#description" >}})

    > [Importance Sampling]({{< relref "20201031155429-importance_sampling.md" >}})
-   [Goal-Conditioned Reinforcement Learning With Imagined Subgoals]({{< relref "20221125112544-goal_conditioned_reinforcement_learning_with_imagined_subgoals.md" >}})

    > This paper
-   [Difference Between MI, IG and KL]({{< relref "20221104141232-difference_between_mi_ig_and_kl.md" >}})

    > What's the difference between [Mutual Information]({{< relref "20221104114029-mutual_information.md" >}}), [Information Gain]({{< relref "20221104103346-information_gain.md" >}}) and [KL Divergence]({{< relref "20201031155541-kl_divergence.md" >}})?
-   [Mutual Information]({{< relref "20221104114029-mutual_information.md" >}})

    > In probability theory and information theory, the mutual information (MI) of two random variables is a measure of the <span class="underline">mutual dependence between the two variables</span>. More specifically, it quantifies the "amount of information" (in units such as shannons (bits), nats or hartleys) obtained about one random variable <span class="underline">by observing the other random variable</span>. The concept of mutual information is intimately linked to that of entropy of a random variable, a fundamental notion in information theory that quantifies the expected "amount of information" held in a random variable.

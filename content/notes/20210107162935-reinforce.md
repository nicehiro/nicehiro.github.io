+++
title = "REINFORCE"
author = ["Fangyuan"]
date = 2021-01-07
slug = "20210107162935-reinforce"
tags = ["rl"]
draft = false
+++

## Description {#description}

在[策略梯度]({{< relref "20210105204602-policy_gradient.md" >}})中，我们提到了策略梯度的目标函数，即：

\\[
J(\theta)=\mathbf{E}\_{\tau\sim p(\tau)}[R(\tau)]
\\]

其梯度为：

\\[
\nabla\_{\theta}J(\theta)=\mathbf{E}\_{\tau\sim\pi(\theta)}[R(\tau)\cdot\nabla\_{\theta}\sum\_{t=0}^{T}\log\pi\_{\theta}(a\_t|s\_t)]
\\]

REINFORCE 算法（又称 Monte-Carlo 梯度估计算法），
是非确定性策略算法，即策略网络输出的是分布，而不是确定的动作。根据分布获得具体的动作。
使用从真实环境中采样的样本去估计 \\(R(\tau)\\) 的期望。

即：

\\[
\mathbf{E}\_{\tau\sim p(\tau)}[R(\tau)]=\lim\_{N\rightarrow\infty}\frac{1}{N}\sum\_{i=1}^{N}R(\tau)
\\]

同理，其梯度也可以用样本去估计，

\\[
\nabla\_{\theta}J(\theta)=\lim\_{N\rightarrow\infty}\frac{1}{N}\sum\_{i=1}^{N}\left[R(\tau)\cdot\nabla\_{\theta}\sum\_{t=0}^{T}\log\pi\_{\theta}(a\_t|s\_t)\right]
\\]

\\(\pi\_{\theta}(a\_t|s\_t)\\) 表示策略在 \\(s\_t\\) 状态执行 \\(a\_t\\) 的概率。

因此，我们得到了最简单的强化学习策略梯度算法：

1.  初始化策略网络
2.  循环
    -   使用策略 \\(\pi\_{\theta}\\) 生成一条轨迹 \\(\tau\\) ，计算其奖励 \\(R(\tau)\\)
    -   更新参数 \\(\theta\leftarrow\theta+\alpha[R(\tau)\cdot\nabla\_{\theta}\sum\_{t=0}^{T}\log\pi\_{\theta}(a\_t|s\_t)]\\)


## Summary {#summary}

**优点** ：

1.  如果当前奖励很大，那么当前轨迹的的概率（或者是当前一连串动作生成的概率）就会变大。
    它能保证轨迹的更新与奖励梯度的方向相同，这就确保了它可以达到局部最优。

**缺点** ：

1.  使用蒙特卡洛采样估计，可能会具有很大的方差，因此学习缓慢。
2.  数据利用率低。作为一种在线（online）学习方法，每一回合的数据只能使用一次。
3.  只能找到局部最优解，很难找到全局最优解。

> Unfortunately, the variance of the gradient estimator scales unfavorably
> with the time horizon, since the effect of an action is confounded with
> the effects of past and future actions.
>
> From GAE paper.


## Reference {#reference}

-   <https://towardsdatascience.com/policy-gradient-methods-104c783251e0>


## Links to this note {#links-to-this-note}

-   [Reinforcement Learning Is Supervised Learning on Optimized Data]({{< relref "20220103205319-rl_is_supervised_learning_on_optimized_data.md" >}})

    > The whole article is interesting. It worth reading regularly.
-   [Actor Critic]({{< relref "20210518141504-actor_critic.md" >}})

    > In [REINFORCE]({{< relref "20210107162935-reinforce.md" >}}), we know that
-   [Off Policy Actor Critic]({{< relref "20210518145521-off_policy_actor_critic.md" >}})

    > One disadvantage of [REINFORCE]({{< relref "20210107162935-reinforce.md" >}}) is low data utilization.

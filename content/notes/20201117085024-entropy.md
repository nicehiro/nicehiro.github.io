+++
title = "Entropy"
author = ["Fangyuan"]
date = 2020-11-17
slug = "20201117085024-entropy"
tags = ["probability"]
draft = false
+++

## Description {#description}

熵（Entropy）是接受的每条信息中包含的信息的平均量。
熵最好理解为不确定性的度量，而不是确定性的度量，因为越随机的信源，熵越大。

**例一** 如果有一枚理想的硬币，其出现正面和反面的机会相等，则抛硬币事件的熵等于其能够达到的最大值。
使用一枚硬币进行 1 次抛投，这个事件的熵是 1 比特（ \\(1\times0.5+1\times0.5\\) ），因为结果不外乎两个——正面或者反面，
可以表示为 0，1 编码，而且两个结果之间相互独立。
若进行 n 次独立实验，则熵为 n。

**例二** 另一个稍微复杂的例子是假设一个随机变量 \\(X\\) ，取三种可能值 \\(x\_1,x\_2,x\_3\\) ，概率分别是 \\(1/2,1/4,1/4\\) ，
那么编码平均比特长度为 \\(1/2\times1+1/4\times2+1/4\times2=3/2\\) ，其熵为 \\(3/2\\) ，
如果 3 个随机变量，第一个随机变量只需要一个比特就可以表示，后两个变量都需要两个比特才能表示。

香农把随机变量 \\(X\\) 的熵值 \\(\mathcal{H}\\) 定义如下，

\\[
H(X)=E[I(X)]=E[-\ln(P(X))]
\\]

其中， \\(P\\) 为 \\(X\\) 的概率函数， \\(E\\) 为期望， \\(I(X)\\) 是 \\(X\\) 的信息量。

当取自有限样本时，熵的公式可以表示为：

\\[
H(X)= \mathbb{E}(I(x\_i)) = \sum\_{i}P(x\_i)I(x\_i)=-\sum\_{i}P(x\_i)\log\_{b}P(x\_i)
\\]

当取自无限样本时，即非连续分布，熵的公式可以表示为：

\begin{equation\*}
H(X) = \int\_{P(x\_i)} I(x\_i)
\end{equation\*}

为什么可以使用 \\(-\log\_{b}P(x)\\) 来表示信息量呢？
我有一个直观未证明的解释。如上 **例二** ， \\(1=-\log\_{2}\frac{1}{2}\\) ， \\(2 = -\log\_{2}\frac{1}{4}\\) 。


## Links to this note {#links-to-this-note}

-   [Pytorch Cross Entropy Loss]({{< relref "20210610144408-pytorch_cross_entropy_loss.md" >}})

    > Preknowledges:
-   [Information Diagram]({{< relref "20221121204537-information_diagram.md" >}})

    > The diagram relationship between [Entropy]({{< relref "20201117085024-entropy.md" >}}), [Conditional Entropy]({{< relref "20221104104233-conditional_entropy.md" >}}), [Mutual Information]({{< relref "20221104114029-mutual_information.md" >}}).
-   [Cross Entropy]({{< relref "20201117092705-cross_entropy.md" >}})

    >   基于相同事件测度的两个概率分布 \\(p\\) 和 \\(q\\) 的交叉熵是指，
    > 当基于一个“非自然”（相对于“真实”分布 \\(p\\) 而言）的概率分布 \\(q\\) 进行编码时，
    > 在事件集合中唯一标识一个事件所需要的平均比特数。
-   [Mutual Information]({{< relref "20221104114029-mutual_information.md" >}})

    > In probability theory and information theory, the mutual information (MI) of two random variables is a measure of the <span class="underline">mutual dependence between the two variables</span>. More specifically, it quantifies the "amount of information" (in units such as shannons (bits), nats or hartleys) obtained about one random variable <span class="underline">by observing the other random variable</span>. The concept of mutual information is intimately linked to that of entropy of a random variable, a fundamental notion in information theory that quantifies the expected "amount of information" held in a random variable.

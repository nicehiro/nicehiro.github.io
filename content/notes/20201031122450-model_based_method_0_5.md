+++
title = "Model Based Method 0.5"
author = ["Fangyuan"]
date = 2020-10-31
slug = "20201031122450-model-based-method-0-5"
tags = ["rl", "mbrl"]
draft = false
+++

## Description {#description}

一旦我们知道了环境动力学模型，那么我们就可以使用前面学到的知识进行动作的选择和规划了。
最 Naive 的方法如下：

<style>.org-center { margin-left: auto; margin-right: auto; text-align: center; }</style>

<div class="org-center">

{{< figure src="/img/rl-sergey/lec-11-1.png" width="100%" >}}

</div>

如果有精心设计的初始策略，以及环境动力学函数（dynamics representation），将会大大提高这种方法的成功性。

但这种方法对于训练集（随机策略探索到的数据）中没有的状态，无法作出很好的判断。
如下图所示，假设随机策略探索到的只有红色线条的部分，它据此构建的模型为“越往右得分越高”。
但真正的模型其实是“往右走时，得分先增高然后减小”。由于探索数据中没有很多的信息导致模型的不准确。

<style>.org-center { margin-left: auto; margin-right: auto; text-align: center; }</style>

<div class="org-center">

{{< figure src="/img/rl-sergey/lec-11-3.png" width="100%" >}}

</div>


## Links to this note {#links-to-this-note}

-   [Model Based RL]({{< relref "20201031101403-model_based_rl.md" >}})

    >   无模型的强化学习不需要知道环境动力学方程，只要不断的进行试错和更新策略的过程就可以很好的进行训练了。
    > 但基于模型的强化学习需要用到环境动力学方程，
    > 在此基础上，对未来进行规划，从而达到加速训练和提高训练结果的效果。

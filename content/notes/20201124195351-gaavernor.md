+++
title = "GAAvernor"
author = ["Fangyuan"]
date = 2020-11-24
slug = "20201124195351-gaavernor"
tags = ["paper"]
draft = false
+++

## Related Works {#related-works}


### GAR {#gar}

从分布式服务器接收到各自的梯度 \\(V\_{i}^{t}\\) 之后，将不同的梯度更新方向整合到一个方向。
通常来说，参数的更新方式为：
\\(\theta\_{t+1}=\theta\_{t}-\lambda\mathcal{F}(V\_{1}^{t},\cdots,V\_{n}^{t})\\)
其中 \\(\lambda\\) 为学习率。


#### Clasical GAR {#clasical-gar}

\\[
\mathcal{F}(V\_{1},\cdots,V\_{n})=\frac{1}{n}\sum\_{i=1}^{n}V\_i
\\]


#### Linear GAR {#linear-gar}

\\[
\mathcal{F}(V\_{1},\cdots,V\_{n})=\sum\_{i=1}^{n}\alpha^{i}V\_i
\\]


### Workers {#workers}


#### Benign Worker {#benign-worker}

如果一个 worker 在时刻 t 的梯度 \\(V^{t}\\) 是无偏的，即
\\(E{V^{t}}=g\_t\\) ，则称此 worker 是 Benign Worker 。


#### Byzantine Worker {#byzantine-worker}

如果一个 worker 在时刻 t 的梯度 \\(V^{t}\\) 是有偏的，即
\\(E{V^{t}}-g\_t\neq 0\\) ，则称此 worker 是 Byzantine Worker 。


### Role Function {#role-function}

用来决定当前 worker 是 Benign 还是 Byzantine 。
如果是 Byzantine Worker，在传送梯度到 server 时会对梯度进行篡改。


### Previous Defenses {#previous-defenses}


#### Brute-Force {#brute-force}

找一个最优集 \\(C^{\*}\\) ，它是 \\(Q\\) 的子集，大小为 \\(n-m\\) ，
\\[
C^{\*}=\arg\min\_{C\in R}\max\_{(V\_i,V\_j)\in C\times C}||V\_i-V\_j||
\\]

GAR 函数为：
\\[
\mathcal{F}(V\_1,\cdots,V\_n)=\frac{1}{n-m}\sum\_{V\in C^{\*}}V
\\]

需要满足 Byzantine ratio： \\(n\geq{2m+1}\\)


#### GeoMed {#geomed}

找与其他 worker 距离之和最小的一个 worker 的梯度作为最终梯度。

GAR 函数为：
\\[
\mathcal{F}(V\_1,\cdots,V\_n):=\arg\min\_{V\_i}\sum\_{j\neq i}||V\_i-V\_j||
\\]

需要满足 Byzantine ratio： \\(n\geq{2m+1}\\)


#### Krum {#krum}

首先利用 Brute-Force GAR 的方法找一个大小为 \\(n-m-2\\) 的最优集，
然后对每个最优集中的 worker，计算：
\\(s(V\_i)=\sum\_{i\rightarrow j}||V\_i-V\_j||^2\\)

GAR 函数为：
\\[
\mathcal{F}(V\_1,\cdots,V\_n)=\arg\min\_{V\_i\in Q}s(V\_i)
\\]

需要满足 Byzantine ratio： \\(n\geq{2m+3}\\)


#### Bulyan {#bulyan}

首先，它运行 Krum 方法得到 \\(n-2m\\) 的梯度的选择集，
然后，它在每个方向上计算 \\(\mathcal{F}\\) ：
\\(\mathcal{F}\\) 的第 i 个坐标等于选择集中第 i 个坐标的中值旁边 \\(n-4m\\) 个值的平均值。

需要满足 Byzantine ratio： \\(n\geq{4m+3}\\)


### Security Assumptions {#security-assumptions}


#### Server 是安全可信的。 {#server-是安全可信的}


#### 至少有一个 worker 不被敌人控制。 {#至少有一个-worker-不被敌人控制}


#### 各个 worker 的数据集是独立同分布的。 {#各个-worker-的数据集是独立同分布的}


#### GAA 有权限接触到验证集。 {#gaa-有权限接触到验证集}


### Goals {#goals}

确保分布式学习可以得到正确的梯度，将损失降到可接受的范围。


### Distributed Learning as a Markov Decision Process {#distributed-learning-as-a-markov-decision-process}


#### States \\(s\_t:=(Q\_{t},\theta\_{t},\hat{f}\_{B}(\theta\_t))\\) {#states-s-t--q-t-theta-t-hat-f-b--theta-t}

\\(\theta\_t\\) 是 Server 的参数， \\(Q\_t\\) 是接收到的梯度，
 \\(\hat{f}\_{B}(\theta\_t)\\) 是 Server 在验证集 \\(B\\) 上的损失。


#### Actions 各个 worker 的权重 {#actions-各个-worker-的权重}


#### Rewards \\(\hat{f}\_{B}(\theta\_t)-\hat^{f}\_{B}(\theta\_{t+1})\\) {#rewards-hat-f-b--theta-t--hat-f-b--theta-t-plus-1}


## Views {#views}


## Methods {#methods}


## Experiments {#experiments}


## Conclusion {#conclusion}

+++
title = "PyTorch Clone"
author = ["Fangyuan"]
date = 2021-10-28
slug = "20211028143342-pytorch-clone"
draft = false
+++

`torch.clone` is differentiable, so gradients will flow back from the result
of this operation to input.

So if you clone the a tensor for backprop, there will have problem:

> RuntimeError: Trying to backward through the graph a second time, but
> the buffers have already been freed. Specify retain_graph=True when
> calling backward the first time.

To create a tensor without an autograd relationship to input see `detach()`.

See details in [doc](https://pytorch.org/docs/stable/generated/torch.clone.html).

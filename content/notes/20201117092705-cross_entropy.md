+++
title = "Cross Entropy"
author = ["Fangyuan"]
date = 2020-11-17
slug = "20201117092705-cross-entropy"
tags = ["math", "probability"]
draft = false
+++

基于相同事件测度的两个概率分布 \\(p\\) 和 \\(q\\) 的交叉熵是指，
当基于一个“非自然”（相对于“真实”分布 \\(p\\) 而言）的概率分布 \\(q\\) 进行编码时，
在事件集合中唯一标识一个事件所需要的平均比特数。

给定两个概率分布 \\(p\\) 和 \\(q\\) ， \\(p\\) 相对于 \\(q\\) 的交叉熵定义为：

\\[
\mathcal{H}(p,q)=E\_{p}[-\log q]
\\]

其中 \\(\mathcal{H}(p)\\) 是 \\(p\\) 的[熵]({{< relref "20201117085024-entropy.md" >}})。

对于离散分布 \\(p\\) 和 \\(q\\) ，
\\[
\mathcal{H}(p,q)=-\sum\_{x}p(x)\log{q(x)}
\\]

大多数情况下，我们需要在不知道分布 \\(p\\) 的情况下计算其交叉熵。
交叉熵的蒙特卡罗估计为：
\\[
\mathcal{H}(T,q)=-\sum\_{i=1}^{N}\frac{1}{N}\log\_{2}q(x\_i)
\\]


## Links to this note {#links-to-this-note}

-   [Pytorch Cross Entropy Loss]({{< relref "20210610144408-pytorch_cross_entropy_loss.md" >}})

    > Preknowledges:
-   [Linear Classification]({{< relref "20201102213824-linear_classification.md" >}})

    > 原文 [Linear Classification](https://cs231n.github.io/linear-classify/)
